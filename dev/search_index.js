var documenterSearchIndex = {"docs":
[{"location":"#BenchmarkDataNLP.jl","page":"Home","title":"BenchmarkDataNLP.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BenchmarkDataNLP.jl is designed to assist users in generating text corpus datasets with parameterized complexity. By utilizing methods such as context-free grammars and Chomsky trees, this package enables the creation of datasets tailored for training natural language processing (NLP) models, including large language models (LLMs). The primary objective is to offer a controlled environment where the complexity of the datasets can be adjusted, facilitating efficient training and evaluation of NLP methods within manageable timeframes and computational resources. Output is .jsonl format producing a training, testing and validation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The vocabulary is synthetic and sampled as is are the grammar rules and word roles/subroles.","category":"page"},{"location":"#Complexity-100-Configuration","page":"Home","title":"Complexity 100 Configuration","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"At complexity = 100, BenchmarkDataNLP.jl uses the following default parameters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Vocabulary: 10,000 words\nGrammar Rules: 200 total\nCharacter Set: 50 letters (characters)\nPunctuation: 10 characters\nMaximum Word Length: 20 characters\nMajor Roles: 50\nPolysemy:\nControlled by a boolean flag enable_polysemy\nIf true, a certain percentage of the vocabulary is allowed to appear in multiple roles.\nLinear Complexity Scaling:\nFrom complexity = 1 up to complexity = 100 (and beyond if desired), the above values scale proportionally allowed up to complexity = 1000 scaling linearly\nFor instance, at lower complexity, you have fewer total words, fewer grammar rules, and shorter maximum word lengths.\nAs complexity increases, the vocabulary, grammar rules, and role/subrole definitions expand, offering progressively more intricate linguistic structures.\nUp to complexity 100 the grammar expansions are more close to resembling normal language (human) constructs producing text that is mostly linguistic. From 101 onward, the expansions are more random to capture arbitrary symbolic or code-like patterns.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This linear scaling ensures that users can move from extremely simple, minimal text corpora up to rich, varied corpora without modifying multiple parameters manually. For example, you might begin at complexity = 1 with very few words and minimal grammatical structures, then gradually progress toward complexity = 100 (and higher) to produce more challenging datasets that test the limits of NLP architectures (up to complexity = 200 is supported and although not clamped larger values may produce saturated data but a hard limit is only put at 1,000).","category":"page"},{"location":"","page":"Home","title":"Home","text":"usage:","category":"page"},{"location":"","page":"Home","title":"Home","text":"generate_corpus_CFG(\n    complexity       = 100,           # Controls grammar, vocab size, etc.\n    num_sentences    = 100_000,       # Number of text samples (lines) to generate for each file of training/testing/validation\n    enable_polysemy  = false,         # Toggle overlap of words among multiple roles\n    output_dir       = \"/home/user/Documents\" # Output path for the files to be saved to\n    base_filename    = \"MyDataset\",   # Base name for output files of .jsonl format (training/testing/validation files of 80%/10%/10% are produced)\n    )","category":"page"},{"location":"#Functions","page":"Home","title":"Functions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [BenchmarkDataNLP]\nPrivate = false\nOrder = [:function]","category":"page"},{"location":"#BenchmarkDataNLP.generate_corpus_CFG-Tuple{}","page":"Home","title":"BenchmarkDataNLP.generate_corpus_CFG","text":"generate_corpus_CFG(; \n    complexity::Int = 100, \n    num_sentences::Int = 100_000, \n    enable_polysemy::Bool = false, \n    output_dir::AbstractString = \".\"\n    base_filename::AbstractString = \"MyDataset\"\n) -> Nothing\n\nGenerate a synthetic corpus of context-free grammar–based text data.\n\nArguments\n\ncomplexity: Controls the grammar complexity, vocabulary size, and other parameters \n\n(e.g., at complexity=100 you might have a 10K-word vocabulary, 200 grammar rules, etc.). After 100 the grammar expansions are less typical of human languages.\n\nnum_sentences: The total number of text samples (e.g., lines or sentences) to generate.\nenable_polysemy: If true, allows words to overlap multiple roles or subroles, introducing \n\nlexical ambiguity in the generated corpus.\n\noutput_dir: The path for the files to be saved to.\nbase_filename: Base name for the output files; the function will typically create files \n\nlike base_filename_training.jsonl, base_filename_validation.jsonl, and  base_filename_test.jsonl depending on how you implement data splitting.\n\nUsage\n\n```julia generatecorpusCFG(     complexity       = 100,     numsentences    = 100000,     enablepolysemy  = false,     outputdir       = \"/home/user/Documents\"     base_filename    = \"MyDataset\" )\n\n\n\n\n\n","category":"method"},{"location":"#BenchmarkDataNLP.generate_fsm_corpus-Tuple{Int64, Int64}","page":"Home","title":"BenchmarkDataNLP.generate_fsm_corpus","text":"generate_fsm_corpus(\n    complexity::Int, \n    num_lines::Int;\n    output_dir::String=\".\",\n    base_name::String=\"MyFSM\",\n    use_context::Bool=false,\n    random_adjacency::Bool=false,\n    max_length::Int=10\n) -> Nothing\n\nBuilds an FSM adjacency according to user options, generates num_lines lines  by walking that adjacency, then splits lines into 80/10/10 train/test/val sets  and writes them to .jsonl files with names like <base_name>_train.jsonl, etc.\n\nKeyword Arguments\n\noutput_dir: Directory for output files (default \".\").\nbase_name: Prefix for output filenames (default \"MyFSM\").\nuse_context: Whether to build a context-based adjacency or not.\nrandom_adjacency: If true, builds a random adjacency. If false, uses a \n\nmore deterministic adjacency approach (example code).\n\nmax_length: Maximum expansions (or steps) in each line's walk (default=10).\n\nUsage\n\n```julia generatefsmcorpus(     50,                # complexity     100;               # produce 100 lines     outputdir=\".\",      basename=\"MyFSM\",     usecontext=true,      randomadjacency=true,     max_length=12 )\n\n\n\n\n\n","category":"method"},{"location":"#BenchmarkDataNLP.generate_rdf_corpus-Tuple{Int64, Int64}","page":"Home","title":"BenchmarkDataNLP.generate_rdf_corpus","text":"generate_rdf_corpus(\n    complexity::Int,\n    num_paragraphs::Int;\n    output_dir::String=\".\",\n    base_name::String=\"MyRDF\",\n    filler_ratio::Float64=0.0,\n    max_filler::Int=0,\n    use_context::Bool=false\n) -> Nothing\n\nGenerate a synthetic RDF-based text corpus and automatically split it into  training, testing, and validation sets. The corpus is saved as .jsonl files.\n\nArguments\n\ncomplexity::Int: Controls the scale of the generated vocabulary and triple store. Higher complexity leads \n\nto a larger vocabulary, more subjects/predicates/objects, and potentially a higher number of triples.\n\nnum_paragraphs::Int: The total number of lines (or “paragraphs” if use_context=true) to \n\nproduce in the final corpus.\n\noutput_dir::String: Directory where output files will be saved (\".\" by default).\nbase_name::String: Base name for the output files. The function will produce three files named \n\n<base_name>_train.jsonl, <base_name>_test.jsonl, and <base_name>_val.jsonl.\n\nfiller_ratio::Float64: Fraction of the vocabulary leftover (after allocating subjects, predicates, \n\nand objects) that is used for filler tokens. For example, a value of 0.3 means 30% of the leftover words      become filler tokens. A higher ratio produces more distinct filler words you can insert in generated sentences.      If this is 0.0, no extra tokens are dedicated to filler.\n\nmax_filler::Int: The maximum number of filler tokens inserted around each subject, \n\npredicate, or object in a generated sentence. For example, if max_filler=2, then up to  two randomly chosen filler tokens might appear before the subject, between subject and  predicate, or between predicate and object.\n\nuse_context::Bool: If true, generates multi-sentence paragraphs reusing previously \n\nmentioned entities (subject/object) within each paragraph, introducing some “context.”  Otherwise, each line is just a single triple-based sentence with no continuity.\n\nDescription\n\nVocabulary & Triple Store: Based on complexity, the function creates a master vocabulary \n\nand partitions it into subjects, predicates, objects, and (optionally) filler. A random subset  of (subject, predicate, object) combinations is then chosen to form a finite triple store.\n\nText Generation:\n\nIf use_context=false, each line is a single sentence referencing a randomly picked triple, \n\noptionally inserting up to max_filler filler tokens around the subject/predicate/object.\n\nIf use_context=true, the function produces multi-sentence paragraphs, where each paragraph \n\nattempts to reuse entities mentioned in prior sentences for added context.\n\nOutput:\n\nThe resulting lines or paragraphs are shuffled and split into training (80%), testing (10%), \n\nand validation (10%) sets.\n\nSaved as JSON lines in files named <base_name>_train.jsonl, <base_name>_test.jsonl, \n\nand <base_name>_val.jsonl within output_dir.\n\nReturns\n\nNothing. The synthetic corpus is written to disk in JSONL format.\n\nExample\n\n```julia generaterdfcorpus(     50,     1000;     outputdir = \".\",     basename = \"MyRDF\",     fillerratio = 0.2,     maxfiller = 2,     usecontext = true )\n\n\n\n\n\n","category":"method"},{"location":"#BenchmarkDataNLP.generate_tps_corpus-Tuple{Int64, Int64}","page":"Home","title":"BenchmarkDataNLP.generate_tps_corpus","text":"generatetpscorpus(     complexity::Int,     numlines::Int;     outputdir::String=\".\",     basename::String=\"MyTPS\",     ntemplates::Int=10,     maxplaceholdersin_template::Int=4,     deterministic::Bool=false )::Nothing\n\nUser-facing function to create a templated corpus, split into train/test/val,  and save as .jsonl.\n\nArguments\n\ncomplexity: Controls vocabulary size. \nnum_lines: How many total lines to generate (will be split 80/10/10).\noutput_dir: Directory for the output JSONL files.\nbase_name: File prefix (e.g. \"MyTPS_train.jsonl\", etc.).\nn_templates: Number of random templates to generate.\nmax_placeholders_in_template: Each template can have up to this many placeholders.\ndeterministic: If true, placeholders are filled in a systematic/round-robin manner. \n\nIf false, placeholders are chosen randomly from the placeholder dictionary.\n\nReturns\n\nNothing. The function writes out train/test/val JSONL files.\n\nUsage\n\ngeneratetpscorpus(50, 100; base_name=\"TemplatedTest\", deterministic=false )\n\n\n\n\n\n","category":"method"},{"location":"#BenchmarkDataNLP.greet-Tuple{}","page":"Home","title":"BenchmarkDataNLP.greet","text":"greet()\n\nPrints a friendly greeting message to the console.\n\nReturns nothing\n\n\n\n\n\n","category":"method"}]
}
